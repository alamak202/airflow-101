# airflow-values.yaml

# Set the executor to KubernetesExecutor
executor: KubernetesExecutor

# Configuration for the Kubernetes Executor
kubernetes:
  # If you have multiple worker node pools, you can specify tolerations and node selectors
  # to schedule task pods on specific nodes.
  workerTolerations: []
  #   - key: "group"
  #     operator: "Equal"
  #     value: "data-processing"
  #     effect: "NoSchedule"y


# env:
#   # static way of defining secrets
#   # - name: AIRFLOW_CONN_MINIO_S3
#   #   value: 'aws://admin:password123@?endpoint_url=http%3A%2F%2Fminio.airflow.svc.cluster.local%3A9000'

#   # defining secrets via k8s secrets
#   - name: AIRFLOW_CONN_MINIO_S3
#     valueFrom:
#       secretKeyRef:
#         # The name of the Secret object we created in Step 1
#         name: airflow-minio-connection-secret
#         # The key within the Secret that holds the value
#         key: connection


# --- NEW (Correct and Secure) ---
# This block allows for the full Kubernetes env var syntax.
# The '|' is important as it defines a multi-line string.
extraEnv: |
  - name: AIRFLOW_CONN_MINIO_S3
    valueFrom:
      secretKeyRef:
        name: airflow-minio-connection-secret
        key: connection


# Resource allocation for the webserver
webserver:
  defaultUser:
    enabled: true
    role: "Admin"
    username: "admin"
    # Set your own password here before installing
    password: "Admin123"
  resources:
    requests:
      cpu: "200m"
      memory: "1Gi"
    limits:
      cpu: "1"
      memory: "3Gi"
  service:
    type: LoadBalancer

# Resource allocation for the scheduler
scheduler:
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "1"
      memory: "2Gi"

# triggerer


triggerer:
  persistence:
    enabled: false


# Configure DAGs to be synced from a git repository
dags:
  gitSync:
    enabled: true
    repo: "https://github.com/alamak202/airflow-101.git" # Replace with your DAGs repository
    branch: "main" # Or your desired branch
    subPath: "dag_examples"
    # If your repository is private, you can use a secret:
    # credentialsSecret: "my-git-credentials-secret"


# --- NEW: Remote Logging Configuration ---
config:
  logging:
    remote_logging: 'True'
    remote_base_log_folder: 's3://airflow/logs'
    remote_log_conn_id: 'minio_s3'
  # connections: # not working
  #   # The value must be a single Airflow Connection URI string.
  #   minio_s3: 's3://admin:password123@minio.airflow.svc.cluster.local:9000'



logs:
  persistence:
    enabled: false
